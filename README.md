# AWS Redshift para Manejo de Big Data

Escuela: üê±‚ÄçüíªData Science
\
Estado: ‚úîÔ∏èFinalizado
\
Fecha de Creaci√≥n: August 3, 2021 4:00 PM

- **Primeros Pasos**
    - **¬øQu√© es un Data Warehouse?**
        - Es una base de datos que recibe informaci√≥n de muchas fuentes de datos.
        - Su objetivo es unificar la informaci√≥n para ayudar a la anal√≠tica de la empresa.
        - Se usa el proceso de ETL (extract, transform, load) para alimentar el Data Warehouse.
        - Extraer: Obtener los datos de las distintas bases de datos
        - Transformar: Realizar una limpieza y modificaci√≥n de los datos, creando una buena estructura anal√≠tica.
        - Cargar: Luego de transformar los datos, se cargar al DW
        
        **¬øQue es una estructura anal√≠tica?**Depende de la organizaci√≥n pero el estandar es el modelo dimensional, que posee:
        
        - Tabla de hechos: Que quiero medir
        - Tabla de dimensiones: Como medirlo, es decir, que variables son importante para generar los an√°lisis
        
        ![https://static.platzi.com/media/user_upload/db_multidimensional%20%281%29-040b6bdc-52fa-456a-bd4d-1753f689ee60.jpg](https://static.platzi.com/media/user_upload/db_multidimensional%20%281%29-040b6bdc-52fa-456a-bd4d-1753f689ee60.jpg)
        
        Otro ejemplo seria en una consulta m√©dica, donde la tabla de hechos es la tabla de consultas.
        
        ![figures/Untitled%201.png](figures/Untitled%201.png)
        
        ![figures/Untitled%202.png](figures/Untitled%202.png)
        
        **Arquitectura del data warehouse**
        ‚óè Tablas de hechos
        
        Contienen la informaci√≥n que queremos
        medir / analizar.
        
        ‚óè Tablas de dimensiones
        
        Contienen la informaci√≥n del ‚Äúc√≥mo‚Äù lo
        quiero medir.
        
        ![figures/Untitled%203.png](figures/Untitled%203.png)
        
        El modelo que usaremos en Redshift:
        
    - **Bases de datos columnares y arquitectura orientada a optimizaci√≥n de consultas**
        - Base de datos relacional: Orientada a la alta transaccionalidad
        - Base de datos columnas: Orientada a la anal√≠tica
        
        Amazon Redshift es un servicio de almac√©n de datos completamente administrado, empresarial y de varios petabytes.
        
        Esta gu√≠a se centra en el uso de Amazon Redshift para crear y administrar un data warehouse. Si trabaja con bases de datos como dise√±ador, desarrollador de software o administrador, le proporciona la informaci√≥n que necesita para dise√±ar, desarrollar, hacer consultas y mantener el data warehouse.
        
        Excelentes para aplicaciones de anal√≠tica.
        ‚óè En redshift cada columna se almacena en bloques de 1 MB.
        ‚óè Una consulta a una tabla de 10 columnas de las cuales solo requiero 2, leer√≠a √∫nicamente las 2 necesarias.
        
        Integraci√≥n total con AWS.
        ‚óè La base de datos m√°s r√°pida en la nube.
        ‚óè Los costos m√°s bajos en la nube.
        ‚óè Alta escalabilidad.
        ‚óè Clientes SQL.
        
        Dbeaver: la usaremos para procesar nuestras DBs en redshift:
        
    - **¬øC√≥mo funciona AWS Redshift?**
        
        El secreto es repartir el trabajo, no se instala en un √∫nico servidor sino que se instala en un cl√∫ster(arreglos de varios servidores conectados **NODOS**)
        
        **Nos conectamos a un nodo l√≠der.**
        
        El cual organiza y asigna las tareas a los otros nodos. (Tareas en paralelos)El nodo seguidor
        
        Y cada nodo seguidor reparte su tarea entre sus Slides. Para trabajar la tarea asignada por el nodo lider en paralelo.
        
        La base de redshift es PostgreSQL.
        
        ![figures/Untitled%204.png](figures/Untitled%204.png)
        
        **Documentaci√≥n:**
        
        [https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html](https://docs.aws.amazon.com/redshift/latest/dg/c_high_level_system_architecture.html)
        
- **Configurando el entorno**
    - **Creando nuestro entorno de trabajo en AWS**
        
        
        [Primero ingresamos a la consola de administraci√≥n de aws](https://us-east-2.console.aws.amazon.com/console/home?region=us-east-2#) y creamos un rol IAM
        
        ![figures/Untitled%205.png](figures/Untitled%205.png)
        
        Ahora creamos un bucket de datos para S3
        
        ![figures/Untitled%206.png](figures/Untitled%206.png)
        
        Un bucket es un contenedor de objetos. Un objeto es un archivo y cualquier metadato que describa ese archivo. Para almacenar un objeto en Amazon S3, cree un bucket y, a continuaci√≥n, cargue el objeto en el bucket.
        
    - **Crear el cluster de trabajo y los nodos**
        
        Dentro de la consola de AWS buscamos redshift y seleccionamos crear cluster
        
        ![figures/Untitled%207.png](figures/Untitled%207.png)
        
        Se puede crear cluster en la versi√≥n trial para aprendizaje con un solo nodo, en el curso usamos un cluster dc2.large de dos nodos, que cuesta con cada TB de transacci√≥n pero por el volumen que trabajaremos en el curso, no tendr√° costo.
        
        Una vez tengamos configurado y disponible nuestro cluster podremos paralelizar nuestros trabajos, cada consulta y labor con la base de datos para trabajar con ella.
        
        ![figures/Untitled%208.png](figures/Untitled%208.png)
        
    - **Consumiendo Redshift: Conectando con un cliente externo de SQL**
        
        Dentro de la consola de AWS primero debemos hacer una serie de configuraciones en las propiedades del cluster que creamos en la clase anterior.
        
        Primero debemos permitir el acceso p√∫blico a nuestro cluster:
        
        ![figures/Untitled%209.png](figures/Untitled%209.png)
        
        E ingresar al VPC security group para obtener reglas de entrada y salida con las que conectaremos
        
        Editamos una regla de entrada y agregamos una con el puerto por default de redshift para cualquier conexi√≥n (para efectos del curso se permite acceso desde cualquier conexi√≥n, pero en la realidad, debemos especificas las IPs que permitiremos.)
        
        ![figures/Untitled%2010.png](figures/Untitled%2010.png)
        
        Regresando al cluster, veremos que en este apartado encontraremos los detalles de conexi√≥n.
        
        ![figures/Untitled%2011.png](figures/Untitled%2011.png)
        
        En este curso usaremos DBeaver como editor SQL y le pasaremos el ODBC URL para conectarnos al cluster (el "enlace" tiene cada uno de los campos que debes ingresar en el formulario de creaci√≥n de la conexi√≥n de DBeaver).
        
        ![figures/Untitled%2012.png](figures/Untitled%2012.png)
        
        Con esto ya podremos ejecutar sentencias SQL dentro de DBeaver conectadas al redwshift
        
        ```sql
        select 'hola mundo!' texto;
        
        create table estudiante(
        id int2,
        nombre varchar(30),
        pais_id int2
        );
        
        create table pais(
        pais_id int2,
        nombre varchar(60)
        );
        
        insert into estudiante values (1,'David',1),(2,'Ange',1),(3,'Felipe',2);
        insert into pais values (1,'Colombia'),(3,'Mexico');
        
        select * from estudiante e
        inner join pais p
        on e.pais_id = p.pais_id;
        
        select * from estudiante e
        left join pais p
        on e.pais_id = p.pais_id;
        
        drop table pais;
        drop table estudiante;
        ```
        
    - **Sentencias SQL en Redshift**
        
        En esta clase realizamos un ejemplo para cargar bases de datos con su informaci√≥n en AWS. Primero usamos unas sentencias SQL en DBeaver para crear las 7 tablas de nuestro proyecto de prueba, el c√≥digo de este proceso es este: [https://github.com/alarcon7a/redshift_course/tree/master/Tickit_db](https://github.com/alarcon7a/redshift_course/tree/master/Tickit_db)
        
        **Creando las tablas:**
        
        ```sql
        create table users(
        	userid integer not null distkey sortkey,
        	username char(8),
        	firstname varchar(30),
        	lastname varchar(30),
        	city varchar(30),
        	state char(2),
        	email varchar(100),
        	phone char(14),
        	likesports boolean,
        	liketheatre boolean,
        	likeconcerts boolean,
        	likejazz boolean,
        	likeclassical boolean,
        	likeopera boolean,
        	likerock boolean,
        	likevegas boolean,
        	likebroadway boolean,
        	likemusicals boolean);
        
        create table venue(
        	venueid smallint not null distkey sortkey,
        	venuename varchar(100),
        	venuecity varchar(30),
        	venuestate char(2),
        	venueseats integer);
        
        create table category(
        	catid smallint not null distkey sortkey,
        	catgroup varchar(10),
        	catname varchar(10),
        	catdesc varchar(50));
        
        create table date(
        	dateid smallint not null distkey sortkey,
        	caldate date not null,
        	day character(3) not null,
        	week smallint not null,
        	month character(5) not null,
        	qtr character(5) not null,
        	year smallint not null,
        	holiday boolean default('N'));
        
        create table event(
        	eventid integer not null distkey,
        	venueid smallint not null,
        	catid smallint not null,
        	dateid smallint not null sortkey,
        	eventname varchar(200),
        	starttime timestamp);
        
        create table listing(
        	listid integer not null distkey,
        	sellerid integer not null,
        	eventid integer not null,
        	dateid smallint not null  sortkey,
        	numtickets smallint not null,
        	priceperticket decimal(8,2),
        	totalprice decimal(8,2),
        	listtime timestamp);
        
        create table sales(
        	salesid integer not null,
        	listid integer not null distkey,
        	sellerid integer not null,
        	buyerid integer not null,
        	eventid integer not null,
        	dateid smallint not null sortkey,
        	qtysold smallint not null,
        	pricepaid decimal(8,2),
        	commission decimal(8,2),
        	saletime timestamp);
        ```
        
        **llaves de distribuci√≥n y ordenamiento distkey sortkey:** Notemos que aparecen dos componentes que no conoc√≠amos antes, estos servir√°n para realizar consultas m√°s efectivas con los IDs claves de la db
        
        Posteriormente deberemos **cargar los datos de cada tabla de manera correspondiente**, en el repositorio de tickitdb existe una carpeta con archivos planos con la informaci√≥n a cargar, este es un ejemplo de allevents_pipe.txt
        
        ```
        1|305|8|1851|Gotterdammerung|2008-01-25 14:30:00
        2|306|8|2114|Boris Godunov|2008-10-15 20:00:00
        3|302|8|1935|Salome|2008-04-19 14:30:00
        4|309|8|2090|La Cenerentola (Cinderella)|2008-09-21 14:30:00
        5|302|8|1982|Il Trovatore|2008-06-05 19:00:00
        6|308|8|2109|L Elisir d Amore|2008-10-10 19:30:00
        7|309|8|1891|Doctor Atomic|2008-03-06 14:00:00
        8|302|8|1832|The Magic Flute|2008-01-06 20:00:00
        ............................................................
        ```
        
        Para cargar los datos debemos subirlos primero a nuestro bucket de S3 (simplemente los arrastramos desde la carpeta)
        
        ![figures/Untitled%2013.png](figures/Untitled%2013.png)
        
        Tengamos presente que la ruta en s3 que se encuentra es s3://nuestrobucket/carpeta/archivo.txt
        
        Finalmente debemos **comunicar cada unade nuestras tablas vac√≠as con sus datos correspondientes en S3**, esto lo haremos con SQL desde DBeaver a trav√©s de t√≠picos copies que realizabamos en PostgreSQL, este es un ejemplo del archivo copy_s3_to_redshift_tickit.sql:
        
        ```sql
        copy users from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/allusers_pipe.txt' 
        credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' 
        delimiter '|' region '<Reemplazar_con_tu_aws_region>';
        
        copy users from 's3://mibucketredshiftsalda/tickitdb/allusers_pipe.txt' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift' 
        delimiter '|' region 'us-east-2';
        ```
        
        Notemos que debemos incluir nuestra ruta s3, nuestro arn rol del cluster y la regi√≥n en la que se encuentra
        
        Ya con esto tendremos finalmente nuestra tablas con los datos, lo comprobamos al realizar queries en DBeaver:
        
        ![figures/Untitled%2014.png](figures/Untitled%2014.png)
        
- **C√≥mo dise√±ar tu base de datos para mejorar su desempe√±o**
    - **¬øQu√© es la compresi√≥n en Redshift? Tipos de codificaci√≥n**
        
        Tengo un archivo con X peso y quiero hacer que pese menos sin perder calidad.
        
        Una tabla en reshift que no este comprimida, esta perdiendo todo el poder.
        
        - Es una operacion dirigida a las columnas.
        - Reduce el tamanio en almacenamiento de los datos reduciendo el I/O.
        - Las consultas son mas veloces en una columna comprimida.
        
        La sentencia que utiliza redshift es la siguiente AL MOMENTO DE CREAR LA TABLA:
        
        CREATE TABLE table_name (column_name data type ENCODE encoding type)
        
        **Ejemplo:**
        
        **CREATE TABLE test_compresion (nombre varchar(30) ENCODE TEXT255)**
        
        **TIPOS DE COMPRENSION:**
        
        - Codificar¬†**RAW**¬†(Predeterminada de Redshift)
            - Los datos se almacenan descomprimidos y sin formato.
            - Tipos da datos: todos.
            - Tipo de codificacion por defecto.
        - Codificacion¬†**AZ64**
            - Codificacion propia de Amazon (AWS).
            - Tipos de dato: smallint, integer, bigint, decimal, date, timestamp, timestamptz.
            - Utiliza SIMD (Single Instruction, Multiple Data) para procesamiento paralelo.
        - Codificacion por¬†**diccionario de bytes**:
            - Muy eficaz cuando una columna tiene una cantidad limitada de valores unicos (menos de 256).
            - Crea un diccionario en un bloque de redshift de 1MB.
            - Tipos de dato: smallint, integer, bigint, decimal, real, double precision, char, varchar, date, timestamp, timestamptz.
        - Codificacion¬†**Delta**:
            - Muy utiles para las columnas con formato fecha y hora.
            - Guarda la diferencia entre un registro y el siguiente.
            - Tipos de datos: smallint, int, bigint, date, timestamp, decimal.
            - Existe Delta de un byte y de dos bytes 8 y 16 respectivamente.
            - No se pueden superar estos bytes en la diferencia, de ser asi la codificacion no se aplica.
            - El rango de 1 byte abarca desde -127 hasta 127 y el rango de 2 bytes desde -32K hasta 32K.
            
            ![figures/Untitled%2015.png](figures/Untitled%2015.png)
            
        - Codificacion¬†**LZO**:
            - Muy util para largas cadenas de texto.
            - Funciona para texto libre.
            - Tipos de dato: smallint, integer, bigint, decimal, char, varchar, date, timestamp, timestamptz.
        - Codificaci√≥n **Mostly:**
            - Se utiliza cuando la mayor√≠a de los datos de una columna son mejores en bits al valor de la columna misma.
            - Existe mostly de 8, 16 y 32 bits.
            - Tipos de dato: smallint, int, bigint, decimal.
        - Codificaci√≥n **Runlength:**
            - Reemplaza un valor que se repite de manera consecutiva por un valor y por un recuento de la cantidad de n√∫meros
            consecutivos.
            - Idea para una tabla en la que los valores de datos suelen repetirse de manera consecutiva.
            - Tipos de dato: smallint, integer, bigint, decimal, real, double precision, boolean, char, varchar, date, timestamp, timestamptz.
            
            ![figures/Untitled%2016.png](figures/Untitled%2016.png)
            
        - Codificaci√≥n **Text255 y Text32k:**
            - Son √∫tiles para comprimir columnas VARCHAR en las que se repiten con frecuencia las mismas palabras.
            - Text255 usa las 245 palabras m√°s frecuentes de la columna y crea un diccionario.
            - Text32k hace lo mismo pero indexa palabras hasta tener un diccionario de 32k.
        - Codificaci√≥n **Zstandard:**
            - La codificaci√≥n ZSTD funciona especialmente bien para las columnas CHAR y VARCHAR.
            - Es muy poco probable que ZSTD aumente el uso del disco.
            - Tipos de dato: smallint, integer, bigint, decimal, real, double, precision, boolean, char, varchar, date, timestamp, timestamptz.
        - **Comparativa de codificaciones**
            
            ![figures/Untitled%2017.png](figures/Untitled%2017.png)
            
        
        [Trabajar con compresi√≥n de columnas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/t_Compressing_data_on_disk.html)
        
    - **Aplicando algoritmos de compresi√≥n**
        
        Vamos a evaluar los algor√≠tmos de compresi√≥n con nuestros datos del proyecto del curso
        
        ```sql
        create  table cartesian_venue as (
        select venueid, venuename, venuecity, venuestate, venueseats
        from venue, listing);
        ```
        
        Realizandole un count(*) esta consulta p√©simamente optimizada da como resultado m√°s de 38 millones de registros.
        
        ![figures/Untitled%2018.png](figures/Untitled%2018.png)
        
        Esta tabla increiblemente grande la usaremos para comparar los distintos m√©todos de compresi√≥n.
        
        Las siguientes sentencias sql nos van a crear versiones codificadas de algunas variables de dicha DB
        
        ```sql
        create table encoding_venue (
        nameraw varchar(100) encode raw,
        namebytedict varchar(100) encode bytedict,
        namelzo varchar(100) encode lzo,
        namerunlength varchar(100) encode runlength,
        nametext255 varchar(100) encode text255,
        nametext32k varchar(100) encode text32k,
        namezstd varchar(100) encode zstd
        );
        
        #crea en encoding_venue una columna con cada m√©todo de compresi√≥n
        insert into encoding_venue
        (
        select venuename, venuename, venuename, venuename, venuename, venuename, venuename
        from cartesian_venue
        );
        ```
        
        ![figures/Untitled%2019.png](figures/Untitled%2019.png)
        
        De esta forma seguimos teniendo nuestros +38 millones de datos en cada columna con su m√©todo de compresi√≥n.
        
    - **An√°lisis de desempe√±o de diferentes tipos de compresi√≥n**
        
        Para esto usaremos tablas propias de redshift
        
        La primara de ellas nos servir√° para obtener el identificador de nuestra tabla
        
        ```sql
        select * from stv_tbl_perm 
        where name = 'encoding_venue';
        ```
        
        ![figures/Untitled%2020.png](figures/Untitled%2020.png)
        
        Este id lo usaremos con la tabla stv_blocklist
        
        ```sql
        select *from pg_catalog.stv_blocklist
        where tbl = 101640;
        ```
        
        ![figures/Untitled%2021.png](figures/Untitled%2021.png)
        
        En el resultado la columna col tendr√° identificados nuestros tipos de encoding, siendo 0:raw,1:Bytedict, 3:lzo, etc.
        
        Ahora realizamos una consulta con los valores m√°ximos en blocknum para ver el espacio que nos ocupa cada encoding.
        
        ```sql
        select col, max(blocknum) from pg_catalog.stv_blocklist
        where tbl = 101640
        and col <= 6 
        #Redshift nos crea columnas adicionales del proceso, 
        #limitamos a los 7 ids de nuestros m√©todos de compresi√≥n
        group by col;
        ```
        
        ![figures/Untitled%2022.png](figures/Untitled%2022.png)
        
        Con esto podemos ver que los mejores m√©todos de compresi√≥n en tama√±o, para nuestro caso, son el bytedict (1: 10) y Zstandard (6: 9); mientras que el m√©todo LZO (3:204) es el peor, llegando incluso a aumentar el espacio de los datos originales.
        
        Vamos a usar una funci√≥n de redshift para analizar la compresi√≥n.
        
        ```sql
        analyze compression cartesian_venue;
        ```
        
        ![figures/Untitled%2023.png](figures/Untitled%2023.png)
        
        Nos entrega el mejor m√©todo de enc√≥ding para cada una de nuestras variables y % de compresi√≥n estimado.
        En este caso, para todas dio como resultado que el mejor es ZSTD, pero puede suceder cada variable tenga un m√©todo de compresi√≥n que mejor le resulte.
        
        **Reto:** hacer el ejercicio de compresi√≥n con venueseats y comparar nuestra conclusi√≥n con el **analyze**
        
        ```sql
        --RETO Venue Seats
        
        create  table cartesian_venueseats as (
        select venueseats
        from venue, listing);
        
        --creamos la lista con los 10 m√©todos de encodings
        create table encoding_venueseats (
        seatsraw integer ENCODE raw,
        seatsaz64 integer ENCODE az64,
        seatsbytedict integer ENCODE bytedict,
        seatsdelta integer ENCODE delta,
        seatsdelta32k integer ENCODE delta32k,
        seatslzo integer ENCODE lzo,
        seatsmostly8 integer ENCODE mostly8,
        seatsmostly16 integer ENCODE mostly16,
        seatsmostly32 bigint ENCODE mostly32,
        seatsrunlength integer ENCODE runlength,
        seatszstd integer ENCODE zstd
        );
        
        insert into encoding_venueseats
        (
        select venueseats, venueseats, venueseats, venueseats, venueseats, venueseats, venueseats
        from cartesian_venueseats
        );
        
        --comprobamos que se insertaran nuestros datos
        select * from encoding_venueseats limit 10;
        
        --buscamos el ID de la tabla, en mi caso es 101656
        select * from stv_tbl_perm 
        where name = 'encoding_venueseats';
        
        --agrupamos los bloqueses de cada m√©todo
        select col, max(blocknum) from pg_catalog.stv_blocklist
        where tbl = 101656
        and col <= 10 
        group by col;
        --en mi an√°lisis ganan zstd y runlength con un peso de 1
        
        --comparamos nuestro an√°lisis con el de redshift
        analyze compression cartesian_venueseats;
        --nos recomienda zstd con una compresi√≥n esperada del 98,99%
        --tiembla Pied Piper! xD
        ```
        
    - **Estilos de distribuci√≥n con Redshift**
        
        ![figures/rtaImage.png](figures/rtaImage.png)
        
        Distribuir carga significa compartir esa carga de trabajo de una tabla de manera equitativa en los nodos, si no est√° distribuida correctamente, unos nodos trabajar√°n m√°s que otros, y eso se traduce en consultas m√°s lentas.
        
        **Distribuci√≥n key:** Determinada columna a mismas locaciones‚Ä¢ Mejora los joins y group by‚Ä¢ Los valores llave se almacenan juntos f√≠sicamente en cada nodo.
        
        **Distribucion ALL:** Todos los datos se replican en cada nodo‚Ä¢ Se usa en tablas peque√±as‚Ä¢ Se distribuyen todos los datos de la tabla en cada nodo.‚Ä¢ Ocupa m√°s espacio en disco y requiere m√°s tiempo para actualizar, eliminar e insertar
        
        **Distrbucion Even:** Distribuci√≥n Roun‚Äôrobin‚Ä¢ √ösala si no es posibl una partici√≥n tipo key‚Ä¢ La data se distribuye en todos los nodos y slides de manera rotativa
        
    - **Evaluando los estilos de distribuci√≥n**
        
        [Art√≠culo de inter√©s The R.A.G](https://dev.to/ronsoak/the-r-a-g-redshift-analyst-guide-things-to-avoid-best-practice-1jbl)
        
        Creamos distintas tablas para revisar como se ven las distribuciones en Redshift y la manera de evaluarlas:
        
        ```sql
        create table dist_key (columna int)
        diststyle key distkey(columna);
        insert into dist_key values (10);
        
        --para even y all no hay necesidad de especificar la columa
        create table dist_even (columna int)
        diststyle even;
        insert into dist_even values (10);
        
        create table dist_all (columna int);
        diststyle all;
        insert into dist_key values (10);
        
        --qu√© pasa si no especificamos
        create table dist_auto (columna int);
        insert into dist_auto values (10);
        
        --usamos una tabla de redshift para ver el tipo de distribuci√≥n
        select * from svv_table_info
        where "table" like '%dist%'
        limit 10;
        -- las auto en casos peque√±os lo deja como all, si es muy grande la vuelve even
        
        --Evaluando buenos estilos de distribuci√≥n
        
        --pg_table_def nos muestra el tipo de distribuci√≥n de cada columna
        select * from pg_table_def
        where tablename = 'users';
        
        --Revisemos el uso de disco de las distribuciones
        select * from pg_catalog.svv_diskusage 
        where "name" = 'users'
        and col = 0
        limit 10;
        --la col 0 es la que tiene la dist
        
        --query para obtener los segmentos de como se distribuyen nuestros datos
        select distinct slice, col, num_values, minvalue, maxvalue from pg_catalog.svv_diskusage 
        where "name" = 'users'
        and col = 0
        and num_values > 0
        order by slice, col;
        --nuestros nodos trabajan de manera equitativa por la cantidad de valores distribuidos entre si
        
        -- creamos una tabla con distribuci√≥n state con los datos de users
        create table user_key_state distkey(state) as (select * from users);
        
        --revisamos como se comporta su distribuci√≥n
        select distinct slice, col, num_values, minvalue, maxvalue from pg_catalog.svv_diskusage 
        where "name" = 'user_key_state'
        and col = 0
        and num_values > 0
        order by slice, col;
        --vemos que en este caso no est√°n equitativos, el nodo 3 es el de menos valores, por mucho
        
        --probamos otra distribuci√≥n: even
        create table user_even diststyle even as (select * from users);
        
        --revisamos como se comporta su distribuci√≥n
        select distinct slice, col, num_values, minvalue, maxvalue from pg_catalog.svv_diskusage 
        where "name" = 'user_even'
        and col = 0
        and num_values > 0
        order by slice, col;
        --aqu√≠ si est√°n distribuidos equitativamente pero est√°n al azar
        --vemos que los valores m√≠n no coinciden en esos slices
        --esto puede generanos anal√≠tica erronea
        
        --finalmente veamos con all
        create table user_all diststyle all as (select * from users);
        
        --revisamos como se comporta su distribuci√≥n
        select distinct slice, col, num_values, minvalue, maxvalue from pg_catalog.svv_diskusage 
        where "name" = 'user_all'
        and col = 0
        and num_values > 0
        order by slice, col;
        --en este caso est√°n perfectamente distribuidos pero ocupan demasiado espacio
        ```
        
        ![figures/Untitled%2024.png](figures/Untitled%2024.png)
        
        ![figures/Untitled%2025.png](figures/Untitled%2025.png)
        
        ![figures/Untitled%2026.png](figures/Untitled%2026.png)
        
        ![figures/Untitled%2027.png](figures/Untitled%2027.png)
        
        ![figures/Untitled%2028.png](figures/Untitled%2028.png)
        
    - **Llaves de ordenamiento para optimizar consultas**
        
        Seg√∫n la llave que utilicemos se realizar√° nuestra consulta y esto significar√° cantidad de recurso
        
        - Clave de clasificaci√≥n compuesta (COMPOUND)
            
            Una clave compuesta se compone de todas las columnas enumeradas en la definici√≥n de clave de clasificaci√≥n, en el orden en que aparecen.¬†**Una clave de clasificaci√≥n compuesta**¬†es m√°s √∫til cuando el filtro de una consulta aplica condiciones, como filtros y combinaciones, que utilizan un prefijo de las claves de clasificaci√≥n. Los beneficios de rendimiento de la clasificaci√≥n compuesta disminuyen cuando las consultas dependen solo de columnas de clasificaci√≥n secundarias, sin hacer referencia a las columnas primarias. COMPOUND es el tipo de clasificaci√≥n predeterminado.
            
            Las claves de ordenaci√≥n compuestas pueden acelerar las combinaciones, las operaciones GROUP BY y ORDER BY, y las funciones de ventana que usan PARTITION BY y ORDER BY. Por ejemplo, una combinaci√≥n de combinaci√≥n, que a menudo es m√°s r√°pida que una combinaci√≥n de hash, es factible cuando los datos se distribuyen y clasifican previamente en las columnas de combinaci√≥n. Las claves de ordenaci√≥n compuestas tambi√©n ayudan a mejorar la compresi√≥n.
            
            A medida que agrega filas a una tabla ordenada que ya contiene datos, la regi√≥n no ordenada crece, lo que tiene un efecto significativo en el rendimiento. El efecto es mayor cuando la tabla utiliza ordenaci√≥n intercalada, especialmente cuando las columnas de ordenaci√≥n incluyen datos que aumentan de manera mon√≥tona, como columnas de fecha o marca de tiempo. Debe ejecutar una operaci√≥n de VAC√çO con regularidad, especialmente despu√©s de grandes cargas de datos, para volver a ordenar y volver a analizar los datos. Para obtener m√°s informaci√≥n, consulte¬†[Administrar el tama√±o de la regi√≥n sin clasificar](https://docs.aws.amazon.com/redshift/latest/dg/r_vacuum_diskspacereqs.html). Despu√©s de pasar la aspiradora para recurrir a los datos, es una buena pr√°ctica ejecutar un comando ANALYZE para actualizar los metadatos estad√≠sticos para el planificador de consultas.
            
        - Clave de ordenaci√≥n intercalada (INTERLEAVED)
            
            Una clave de ordenaci√≥n intercalada le otorga el mismo peso a cada columna o subconjunto de columnas en la clave de ordenaci√≥n. Si hay distintas consultas que usan diferentes columnas para filtros, puede, por lo general, mejorar el rendimiento de esas consultas utilizando un estilo de ordenaci√≥n intercalada. Cuando una consulta usa predicados restrictivos en las columnas de ordenaci√≥n secundarias, la ordenaci√≥n intercalada mejora considerablemente el rendimiento de la consulta, si se la compara con la ordenaci√≥n compuesta
            
        - Art√≠culo de AWS: Choose the best sort key
            - To have Amazon Redshift choose the appropriate sort order, specify¬†`AUTO`¬†for the sort key.
            - If recent data is queried most frequently, specify the timestamp column as the leading column for the sort key.
                
                Queries are more efficient because they can skip entire blocks that fall outside the time range.
                
            - If you do frequent range filtering or equality filtering on one column, specify that column as the sort key.
                
                Amazon Redshift can skip reading entire blocks of data for that column. It can do so because it tracks the minimum and maximum column values stored on each block and can skip blocks that don't apply to the predicate range.
                
            - If you frequently join a table, specify the join column as both the sort key and the distribution key.
                
                Doing this enables the query optimizer to choose a sort merge join instead of a slower hash join. Because the data is already sorted on the join key, the query optimizer can bypass the sort phase of the sort merge join.
                
                [Choose the best sort key](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)
                
    - **Aplicando ordenamiento de columnas**
        
        ```sql
        --Creamos los datos con el repo de archivos del curso (BZ2) tipo de compresi√≥n de redshift
        --recuerda que debemos llevarlo a nuestro bucket en s3
        
        --posteriormente creamos una tabla a la que le conectaremos los datos del BZ2
        CREATE TABLE public.cust_sales_date (
        	c_custkey int4 NULL,
        	c_nation varchar(15) NULL,
        	c_region varchar(12) NULL,
        	c_mktsegment varchar(10) NULL,
        	d_date date NULL,
        	lo_revenue int4 NULL
        );
        
        --la instrucci√≥n BZIP2 le especifica las caracteristicas del archivo comprimido
        copy cust_sales_date from 's3://mibucketredshiftsalda/cust_sales_date_000.bz2' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift' 
        BZIP2
        region 'us-east-2';
        
        --vemos nuestros datos
        select count(*) from cust_sales_date; 
        select * from cust_sales_date limit 10;
        
        --creamos una tabla auxiliar de 5 valores para que sea muchos m√°s datos
        create table auxiliar (col int);
        insert into auxiliar values (1), (2), (3), (4), (5);
        select * from auxiliar;
        
        --multiplicamos ambas tablas usando la TERRIBLE pr√°ctica
        select c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue
        from cust_sales_date, auxiliar
        limit 5;
        
        --crearemos tablas con cada llave de ordenamiento
        
        --Simple Key
        create table cust_sales_simple
        sortkey(c_custkey)
        as (select  c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue
        from cust_sales_date, auxiliar
        );
        
        --consultamos Simple Key
        select count(*) from cust_sales_simple;
        
        --Compound Key
        create table cust_sales_compuesto
        compound sortkey(c_custkey, c_Region, c_mktsegment, d_date)
        as (select  c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue
        from cust_sales_date, auxiliar
        );
        --toma m√°s tiempo que el simple ya que son m√°s elementos de ordenamiento
        
        --consultamos Compound Key
        select count(*) from cust_sales_compuesto;
        
        --interleaved Key
        create table cust_sales_intercalado
        interleaved sortkey(c_custkey, c_Region, c_mktsegment, d_date)
        as (select  c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue
        from cust_sales_date, auxiliar
        );
        --toma much√≠simo m√°s tiempo que los anteriores pero es m√°s preciso en el ordenamiento
        
        --consultamos interleaved Key
        select count(*) from cust_sales_intercalado;
        ```
        
        ![figures/Untitled%2029.png](figures/Untitled%2029.png)
        
        ![figures/Untitled%2030.png](figures/Untitled%2030.png)
        
    - **Evaluando algoritmos de ordenamiento**
        
        Usaremos las tablas creadas para evaluar los algor√≠timos de ordenamiento 
        
        ```sql
        set enable_result_cache_for_session to off;
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_simple
        WHERE c_custkey < 100000; --349ms ,237ms
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_compuesto
        WHERE c_custkey < 100000; --125ms ,146ms
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_intercalado
        WHERE c_custkey < 100000; --397 ms ,179ms
        -------------------------------
        
        set enable_result_cache_for_session to off;
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_simple
        WHERE c_region = 'ASIA'
        AND c_mktsegment = 'FURNITURE'; --1.468ms, 1.369 ms
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_compuesto
        WHERE c_region = 'ASIA'
        AND c_mktsegment = 'FURNITURE';--2.586s, 770ms
        
        SELECT max(lo_revenue), min(lo_revenue) 
        FROM cust_sales_intercalado
        WHERE c_region = 'ASIA'
        AND c_mktsegment = 'FURNITURE';--303ms, 104 ms
        
        -----------
        set enable_result_cache_for_session to off;
        
        SELECT max(lo_revenue), min(lo_revenue)
        FROM cust_sales_simple
        WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'
        AND c_mktsegment = 'FURNITURE'
        AND c_region = 'ASIA';--1.415s, 115ms
        
        SELECT max(lo_revenue), min(lo_revenue)
        FROM cust_sales_compuesto
        WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'
        AND c_mktsegment = 'FURNITURE'
        AND c_region = 'ASIA';--806 ms, 326ms
        
        SELECT max(lo_revenue), min(lo_revenue)
        FROM cust_sales_intercalado
        WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'
        AND c_mktsegment = 'FURNITURE'
        AND c_region = 'ASIA';--236 ms, 112ms
        ```
        
    - **Buenas pr√°cticas para dise√±ar tablas en Redshift**
        - Usa siempre tipos de datos date o timestamp para las fechas, si es necesario ajusta tus procesos ETL.
        - Tama√±os muy elevados degradan el uso de algoritmos de compresi√≥n y requieren m√°s almacenamiento en disco.
        - Comenta en qu√© consisten tus columnas en el diccionario de datos.
        
        Redshift en la documentaci√≥n oficial tiene una query para analizarel desempe√±o de nuestrs tablas
        
        ```sql
        select
        	schema schemaname,
        	"table" tablename,
        	table_id tableid,
        	size size_in_mb,
        	case
        		when diststyle not in ('EVEN', 'ALL') then 1
        		else 0
        	end has_dist_key,
        	case
        		when sortkey1 is not null then 1
        		else 0
        	end has_sort_key,
        	case
        		when encoded = 'Y' then 1
        		else 0
        	end has_col_encoding,
        	cast(max_blocks_per_slice - min_blocks_per_slice as FLOAT) / greatest(NVL (min_blocks_per_slice, 0)::int, 1) ratio_skew_across_slices,
        	cast(100 * dist_slice as FLOAT) /(
        	select
        		COUNT(distinct slice)
        	from
        		stv_slices) pct_slices_populated
        from
        	svv_table_info ti
        join (
        	select
        		tbl, MIN(c) min_blocks_per_slice, MAX(c) max_blocks_per_slice, COUNT(distinct slice) dist_slice
        	from
        		(
        		select
        			b.tbl, b.slice, COUNT(*) as c
        		from
        			STV_BLOCKLIST b
        		group by
        			b.tbl, b.slice)
        	where
        		tbl in (
        		selectselect
        	schema schemaname,
        	"table" tablename,
        	table_id tableid,
        	size size_in_mb,
        	case
        		when diststyle not in ('EVEN', 'ALL') then 1
        		else 0
        	end has_dist_key,
        	case
        		when sortkey1 is not null then 1
        		else 0
        	end has_sort_key,
        	case
        		when encoded = 'Y' then 1
        		else 0
        	end has_col_encoding,
        	cast(max_blocks_per_slice - min_blocks_per_slice as FLOAT) / greatest(NVL (min_blocks_per_slice, 0)::int, 1) ratio_skew_across_slices,
        	cast(100 * dist_slice as FLOAT) /(
        	select
        		COUNT(distinct slice)
        	from
        		stv_slices) pct_slices_populated
        from
        	svv_table_info ti
        join (
        	select
        		tbl, MIN(c) min_blocks_per_slice, MAX(c) max_blocks_per_slice, COUNT(distinct slice) dist_slice
        	from
        		(
        		select
        			b.tbl, b.slice, COUNT(*) as c
        		from
        			STV_BLOCKLIST b
        		group by
        			b.tbl, b.slice)
        	where
        		tbl in (
        		select
        			table_id
        		from
        			svv_table_info)
        	group by
        		tbl) iq on
        	iq.tbl = ti.table_id;
        			table_id
        		from
        			svv_table_info)
        	group by
        		tbl) iq on
        	iq.tbl = ti.table_id;
        ```
        
        Nos indica nuestra distribuci√≥n, uso de slices y posibles sesgos de nuestras tablas.
        
    - **Tipos de datos en AWS Redshift**
        
        En esta clase conoceremos los tipos de datos que maneja Redshift, y como ya lo sabes, Redshift est√° basado en PostgreSQL, de manera que si est√°s familiarizado con los tipos de datos de PostgreSQL esta clase se te facilitar√°.
        
        Como otras bases de datos, Redshift maneja datos de tipo num√©rico, texto, de l√≥gica booleana y estos datos tienen ciertas particularidades.
        
        **Tipos de datos para texto:**
        
        En estos tipos de datos almacenaremos caracteres y cadenas de texto como pueden ser nombres, direcciones, descripciones, etc.
        
        Lo m√°s importante que debes saber al momento de manejar estos tipos de datos es que el par√°metro que le indiques a cada dato no es la longitud, es el n√∫mero de bytes que trabajar√°, de manera que un dato VARCHAR(2) no indica que reciba 2 caracteres de longitud, sino que el contenido es de 2 bytes.
        
        ```sql
        CREATETABLE  test_tipo_dato (
        dato  varchar(2)
        )
        
        insertinto  test_tipo_datovalues ('AA');
        insertinto  test_tipo_datovalues ('‚Ç¨');
        
        ```
        
        Si ejecutas las anteriores sentencias notar√°s que el primer insert se ejecuta sin ning√∫n problema, ‚ÄúA‚Äù es un car√°cter que solo ocupa 1 byte. De modo que ‚ÄúAA‚Äù ocupa un total de 2 bytes. Pero, la segunda sentencia de insert fall√≥ y esta solo contiene un car√°cter ‚Äú‚Ç¨‚Äù, esto ocurre por que este car√°cter tiene una longitud de 3 bytes por s√≠ solo, de manera que rompe la restricci√≥n de tipo de dato VARCHAR(2).
        
        ![https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.05.25-2a5f36ba-4144-4c1b-b190-4796150a9b7d.jpg](https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.05.25-2a5f36ba-4144-4c1b-b190-4796150a9b7d.jpg)
        
        Como ves los tipos de dato BPCHAR son convertidos por Redshift a CHAR y los tipos de dato TEXT a VARCHAR; tambi√©n es importante tener en cuenta que un dato CHAR de 10, siempre ocupara 10 bytes, y dado el caso que el dato guardado no complete los 10 bytes Redshift lo completa con espacios en blanco, esto no ocurre en el tipo de dato VARCHAR.
        
        **Compru√©balo t√∫ mismo:**
        
        ```sql
        createtable  test_char (
        dato  char(10)
        );
        
        insertinto  test_charvalues ('dato');
        select *from  test_char;
        
        ```
        
        **Tipos de datos num√©ricos:**
        
        Estos tipos de datos se deben usar para el manejo de n√∫meros, en casos como cantidades, medidas, llaves √∫nicas etc, los datos num√©ricos pueden ser enteros, decimales y de coma flotante.
        
        **Enteros:**
        
        ![https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.07.07-9ae031f7-855e-4dee-96ed-d93d822e20b6.jpg](https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.07.07-9ae031f7-855e-4dee-96ed-d93d822e20b6.jpg)
        
        **Decimales:**
        
        ![https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.07.29-d3366c25-787d-4e98-ab84-e314eb20e517.jpg](https://static.platzi.com/media/user_upload/CleanShot%202020-08-09%20at%2020.07.29-d3366c25-787d-4e98-ab84-e314eb20e517.jpg)
        
        Para el manejo de datos decimales, decimal(precisi√≥n, escala), debes tener en cuenta que se manejan dos par√°metros, la precisi√≥n y la escala.
        
        Para explicarte como funciona usemos un ejemplo:
        
        ```sql
        createtable  test_decimal (
        dato  decimal(4,2)
        );
        
        insertinto  test_decimalvalues (4581.54);
        insertinto  test_decimalvalues (121.7);
        insertinto  test_decimalvalues (12.21);
        insertinto  test_decimalvalues (12.2);
        insertinto  test_decimalvalues (12.21222);
        
        ```
        
        Como puedes observar los dos primeros ‚Äúinsert‚Äù fallaron, y quiz√° te est√©s preguntando, ¬øpor qu√© ocurri√≥ esto? si en la sentencia dice 4 enteros y 2 decimales‚Ä¶ pero no, el valor de precisi√≥n (4 para este ejemplo) indica el n√∫mero total de d√≠gitos que puede recibir, y escala indica el n√∫mero de decimales que se trabajar√°n, de modo que en realidad este dato puede m√°ximo guardar un n√∫mero de 2 enteros y 2 decimales (4,2), pero ¬øy la √∫ltima sentencia insert que ten√≠a 2 enteros y 5 decimales? ¬øpor qu√© funcion√©?, bueno Redshift nota que el n√∫mero de enteros es v√°lido y solo redondea las cifras decimales a 2 d√≠gitos para este caso.
        
        **Otros tipos de dato:**
        
        Los valores de fecha como date, timestamp, timestampz se manejan como cualquier otra base de datos, tambi√©n los tipos de dato bool, boolean, los cuales solo pueden recibir un byte de longitud.
        
        A continuaci√≥n te comparto la tabla que usa Redshift para el manejo de tipos de datos.
        
        ![https://static.platzi.com/media/user_upload/datos-redshift-0c6df4b0-5cf9-46f1-b1de-0a66cad0165b.jpg](https://static.platzi.com/media/user_upload/datos-redshift-0c6df4b0-5cf9-46f1-b1de-0a66cad0165b.jpg)
        
    - **Reto: mejora el desempe√±o de tu base de datos**
        
        Tomar una nueva DB cruda y aplicar los m√©todos de compresi√≥n para que los queries del reto se ejecuten de manera √≥ptima
        
        **[repo del reto](https://github.com/alarcon7a/redshift_course/blob/master/Reto_Dise%C3%B1o_Tablas/Creacion_Tablas.sql)**
        
        Parte inicial: Conectar y revisar la tabla customer
        
        ```sql
        copy customer from 's3://awssampledbuswest2/ssbgz/customer' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'  
        gzip compupdate off region 'us-west-2';
        
        select * from customer limit 10;
        
        analyze compression customer;
        ```
        
        ![figures/Untitled%2031.png](figures/Untitled%2031.png)
        
        El resultado nos indica los tipos de compresi√≥n que deber√≠a tener cada variable, veamos de qu√© manera lo cre√≥ por default
        
        Esto lo hacemos en la opci√≥n DDL al verl la tabla dentro de la lista de todas ellas con F4.
        
        ![figures/Untitled%2032.png](figures/Untitled%2032.png)
        
        Podemos usar esa sentencia SQL para modificarla con los resultados que recomienda el an√°lisis.
        
        Recuerda que aqu√≠ est√° creada la tabla con los tipos de compresi√≥n, la llave de distribuci√≥n pero necesita los datos
        
        ```sql
        CREATE TABLE IF NOT EXISTS public.customer_pro
        (
        	c_custkey INTEGER NOT NULL  ENCODE az64 distkey
        	,c_name VARCHAR(25) NOT NULL  ENCODE zstd
        	,c_address VARCHAR(25) NOT NULL  ENCODE zstd
        	,c_city VARCHAR(10) NOT NULL  ENCODE bytedict
        	,c_nation VARCHAR(15) NOT NULL  ENCODE bytedict
        	,c_region VARCHAR(12) NOT NULL  ENCODE bytedict
        	,c_phone VARCHAR(15) NOT NULL  ENCODE zstd
        	,c_mktsegment VARCHAR(10) NOT NULL  ENCODE bytedict
        );
        ALTER TABLE public.customer owner to platzi_redshift;
        
        insert into customer_pro (select * from customer);
        ```
        
        ![figures/Untitled%2033.png](figures/Untitled%2033.png)
        
        El reto es aplicar la optimizaci√≥n para las dem√°s tablas y ponerlas a prueba con los queries de sentencias en el repo del curso, mediremos tiempos de respuesta y calidad de datos de la versi√≥n original y de nuestra mejorada.
        
        Challenge Progress:
        
        ```sql
        set enable_result_cache_for_session to off;
        
        --copiamos las tablas del bucket original
        copy dwdate from 's3://awssampledbuswest2/ssbgz/dwdate' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'  
        gzip compupdate off region 'us-west-2';
        
        copy lineorder from 's3://awssampledbuswest2/ssbgz/lineorder' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'  
        gzip compupdate off region 'us-west-2';
        
        copy part from 's3://awssampledbuswest2/ssbgz/part' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'  
        gzip compupdate off region 'us-west-2';
        
        copy supplier from 's3://awssampledbuswest2/ssbgz/supplier' 
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'  
        gzip compupdate off region 'us-west-2';
        
        --analizamos cada una de las tablas para identificar su compresi√≥n recomendada
        --y creamos nuestra versi√≥n pro de cada una con los datos de su versi√≥n original
        
        --dwdate
        analyze compression dwdate;
        
        CREATE TABLE IF NOT EXISTS public.dwdate_pro
        (
        	d_datekey INTEGER NOT NULL  ENCODE az64 distkey
        	,d_date VARCHAR(19) NOT NULL  ENCODE lzo
        	,d_dayofweek VARCHAR(10) NOT NULL  ENCODE lzo
        	,d_month VARCHAR(10) NOT NULL  ENCODE lzo
        	,d_year INTEGER NOT NULL  ENCODE az64
        	,d_yearmonthnum INTEGER NOT NULL  ENCODE az64
        	,d_yearmonth VARCHAR(8) NOT NULL  ENCODE lzo
        	,d_daynuminweek INTEGER NOT NULL  ENCODE az64
        	,d_daynuminmonth INTEGER NOT NULL  ENCODE az64
        	,d_daynuminyear INTEGER NOT NULL  ENCODE az64
        	,d_monthnuminyear INTEGER NOT NULL  ENCODE az64
        	,d_weeknuminyear INTEGER NOT NULL  ENCODE az64
        	,d_sellingseason VARCHAR(13) NOT NULL  ENCODE lzo
        	,d_lastdayinweekfl VARCHAR(1) NOT NULL  ENCODE lzo
        	,d_lastdayinmonthfl VARCHAR(1) NOT NULL  ENCODE lzo
        	,d_holidayfl VARCHAR(1) NOT NULL  ENCODE lzo
        	,d_weekdayfl VARCHAR(1) NOT NULL  ENCODE lzo
        );
        ALTER TABLE public.dwdate_pro owner to platzi_redshift;
        
        insert into dwdate_pro (select * from dwdate);
        
        analyze compression dwdate_pro;
        
        --lineorder
        analyze compression lineorder;
        
        CREATE TABLE IF NOT EXISTS public.lineorder_pro
        (
        	lo_orderkey INTEGER NOT NULL  ENCODE zstd distkey
        	,lo_linenumber INTEGER NOT NULL  ENCODE az64
        	,lo_custkey INTEGER NOT NULL  ENCODE az64
        	,lo_partkey INTEGER NOT NULL  ENCODE az64
        	,lo_suppkey INTEGER NOT NULL  ENCODE az64
        	,lo_orderdate INTEGER NOT NULL  ENCODE az64
        	,lo_orderpriority VARCHAR(15) NOT NULL  ENCODE bytedict
        	,lo_shippriority VARCHAR(1) NOT NULL  ENCODE zstd
        	,lo_quantity INTEGER NOT NULL  ENCODE az64
        	,lo_extendedprice INTEGER NOT NULL  ENCODE zstd
        	,lo_ordertotalprice INTEGER NOT NULL  ENCODE zstd
        	,lo_discount INTEGER NOT NULL  ENCODE az64
        	,lo_revenue INTEGER NOT NULL  ENCODE zstd
        	,lo_supplycost INTEGER NOT NULL  ENCODE az64
        	,lo_tax INTEGER NOT NULL  ENCODE az64
        	,lo_commitdate INTEGER NOT NULL  ENCODE az64
        	,lo_shipmode VARCHAR(10) NOT NULL  ENCODE bytedict
        );
        ALTER TABLE public.lineorder_pro owner to platzi_redshift;
        
        insert into lineorder_pro (select * from lineorder);
        analyze compression lineorder_pro;
        
        -- part
        analyze compression part;
        
        CREATE TABLE IF NOT EXISTS public.part_pro
        (
        	p_partkey INTEGER NOT NULL  ENCODE delta distkey
        	,p_name VARCHAR(22) NOT NULL  ENCODE text255
        	,p_mfgr VARCHAR(6) NOT NULL  ENCODE zstd
        	,p_category VARCHAR(7) NOT NULL  ENCODE bytedict
        	,p_brand1 VARCHAR(9) NOT NULL  ENCODE zstd
        	,p_color VARCHAR(11) NOT NULL  ENCODE bytedict
        	,p_type VARCHAR(25) NOT NULL  ENCODE bytedict
        	,p_size INTEGER NOT NULL  ENCODE az64
        	,p_container VARCHAR(10) NOT NULL  ENCODE bytedict
        );
        ALTER TABLE public.part_pro owner to platzi_redshift;
        
        insert into part_pro (select * from part);
        analyze compression part_pro;
        
        --supplier
        analyze compression supplier;
        
        --drop table supplier_pro;
        
        CREATE TABLE IF NOT EXISTS public.supplier_pro
        (
        	s_suppkey INTEGER NOT NULL  ENCODE delta distkey
        	,s_name VARCHAR(25) NOT NULL  ENCODE zstd
        	,s_address VARCHAR(25) NOT NULL  ENCODE zstd
        	,s_city VARCHAR(10) NOT NULL  ENCODE bytedict
        	,s_nation VARCHAR(15) NOT NULL  ENCODE bytedict
        	,s_region VARCHAR(12) NOT NULL  ENCODE bytedict
        	,s_phone VARCHAR(15) NOT NULL  ENCODE zstd
        );
        ALTER TABLE public.supplier owner to platzi_redshift;
        
        insert into supplier_pro (select * from supplier);
        analyze compression supplier_pro;
        
        --Es hora de probar los queries
        
        -- Query 1
        --tiempo raw: 6.280s
        select sum(lo_extendedprice*lo_discount) as revenue
        from lineorder, dwdate
        where lo_orderdate = d_datekey
        and d_year = 1997
        and lo_discount between 1 and 3
        and lo_quantity < 24;
        
        --tiempo pro: 7.897s
        select sum(lo_extendedprice*lo_discount) as revenue
        from lineorder_pro, dwdate_pro
        where lo_orderdate = d_datekey
        and d_year = 1997
        and lo_discount between 1 and 3
        and lo_quantity < 24;
        
        -- Query 2
        --tiempo raw: 10.596
        select sum(lo_revenue), d_year, p_brand1
        from lineorder, dwdate, part, supplier
        where lo_orderdate = d_datekey
        and lo_partkey = p_partkey
        and lo_suppkey = s_suppkey
        and p_category = 'MFGR#12'
        and s_region = 'AMERICA'
        group by d_year, p_brand1
        order by d_year, p_brand1;
        
        --tiempo pro: 18.819s
        select sum(lo_revenue), d_year, p_brand1
        from lineorder_pro, dwdate_pro, part_pro, supplier_pro
        where lo_orderdate = d_datekey
        and lo_partkey = p_partkey
        and lo_suppkey = s_suppkey
        and p_category = 'MFGR#12'
        and s_region = 'AMERICA'
        group by d_year, p_brand1
        order by d_year, p_brand1;
        
        -- Query 3
        --tiempos raw: 5.474
        select c_city, s_city, d_year, sum(lo_revenue) as revenue
        from customer, lineorder, supplier, dwdate
        where lo_custkey = c_custkey
        and lo_suppkey = s_suppkey
        and lo_orderdate = d_datekey
        and (c_city='UNITED KI1' or
        c_city='UNITED KI5')
        and (s_city='UNITED KI1' or
        s_city='UNITED KI5')
        and d_yearmonth = 'Dec1997'
        group by c_city, s_city, d_year
        order by d_year asc, revenue desc;
        
        --tiempo pro: 18.327
        select c_city, s_city, d_year, sum(lo_revenue) as revenue
        from customer_pro, lineorder_pro, supplier_pro, dwdate_pro
        where lo_custkey = c_custkey
        and lo_suppkey = s_suppkey
        and lo_orderdate = d_datekey
        and (c_city='UNITED KI1' or
        c_city='UNITED KI5')
        and (s_city='UNITED KI1' or
        s_city='UNITED KI5')
        and d_yearmonth = 'Dec1997'
        group by c_city, s_city, d_year
        order by d_year asc, revenue desc;
        
        -- Query 4
        --tiempos raw: 17.680
        select sum(lo_revenue) , c.c_nation from lineorder l 
        inner join customer c 
        on l.lo_custkey = c.c_custkey 
        where c_region  = 'EUROPE'
        group by c.c_nation
        order by 1 desc;
        
        --tiempos pro: 14.6980
        select sum(lo_revenue) , c.c_nation from lineorder_pro l 
        inner join customer c 
        on l.lo_custkey = c.c_custkey 
        where c_region  = 'EUROPE'
        group by c.c_nation
        order by 1 desc;
        ```
        
- **Manipular enormes cantidades de datos**
    - **Olv√≠date de los insert, el copy llego para quedarse**
        
        **Comando Copy**
        
        - Procesamiento masivo en paralelo (MPP)
        - Un solo llamado para m√∫ltiples archivos.
        - Compresi√≥n de archivos (Cargar archivos comprimidos)
        - Compatible con s3.
        
        **Carga de archivos**
        
        - Otorgar permisos al recurso (A trav√©s del rol)
        - Validar formato de archivos (Debe de estar en UTF-8).
        - Verificar longitudes de las columnas.
        - ¬øExiste un delimitador?
        - Comprobar el formato de fechas.
        - Particionar los datos en distintos archivos.
        
        **¬øEn cuantos archivos debo dividir mis datos?**Cargar archivos m√∫ltiplos del numero de slide por nodo.2 slides, 2 nodos -> 4 archivos.Recomendaci√≥n de tama√±o: 110 MB por archivo despu√©s de la compresi√≥n.
        
    - **El comando copy a fondo** + **Manifiestos y COMPUPDATE para carga con compresi√≥n autom√°tica**
        
        Para este caso trabajaremos con estos archivos del [repo](https://github.com/alarcon7a/redshift_course/tree/master/Copy) para la primera parte de copy
        
        y con la el repo de [Manifest](https://github.com/alarcon7a/redshift_course/tree/master/Copy/Manifest) para la segunda.
        
        El comando fixedwidth indica el numero de columna + dos puntos + numero de caracteres utilizados.
        Un archivo de manifiesto indica donde se desea cargar los archivos.
        
        ```sql
        --primero debemos crear una tabla que soporte la estructura de los CSV
        create table estudiante
        (
        id int2,
        nombre varchar(20),
        apellido varchar(20),
        edad int2,
        fecha_ingreso date
        );
        
        select * from estudiante;
        
        copy estudiante from 's3://mibucketredshiftsalda/primer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2';
        
        --nos da un error de cargue, verificamos:
        select * from stl_load_errors;
        --nos muestra el fallo y que le falt√≥ especificar el delimitador
        
        --cargamos una vez m√°s
        copy estudiante from 's3://mibucketredshiftsalda/primer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';';
        
        --nos da otro error, vamos a consultarlo
        select * from stl_load_errors;
        --la db tiene encabezado
        copy estudiante from 's3://mibucketredshiftsalda/primer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';'
        ignoreheader 1;
        
        --nuevamente falla por las l√≠neas en blanco
        select * from stl_load_errors;
        
        copy estudiante from 's3://mibucketredshiftsalda/primer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';'
        ignoreheader 1
        ignoreblanklines;
        
        --finalmente se cargaron los datos
        select * from estudiante;
        --pero vemos que hay algunos nombres en blanco
        
        --borraremos la data de la tabla
        truncate table estudiante;
        
        --y cargamos nuevamente con una configuraci√≥n m√°s
        copy estudiante from 's3://mibucketredshiftsalda/primer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';'
        ignoreheader 1
        ignoreblanklines
        blanksasnull;
        --esto nos generar√° nuestros datos en blanco como nulls
        select * from estudiante;
        
        --ahora intentemos esto con otro archivo
        --en el segundo el formato de fecha es diferente
        copy estudiante from 's3://mibucketredshiftsalda/segundo_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';'
        ignoreheader 1
        ignoreblanklines
        blanksasnull;
        --efectivamente nos dice que el formato de fecha no es v√°lido
        select * from stl_load_errors;
        
        copy estudiante from 's3://mibucketredshiftsalda/segundo_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        delimiter ';'
        ignoreheader 1
        ignoreblanklines
        blanksasnull
        dateformat 'mm-dd-yyyy';
        
        select * from estudiante;
        --lo interesante es que los formatos de fecha son diferentes en cargue
        --pero iguales en las consultas
        
        --SEGUNDA PARTE MANIFEST Y COMPUPDATE
        
        --Ahora cargaremos el tercero que no tienen ning√∫n delimitador
        copy estudiante from 's3://mibucketredshiftsalda/tercer_cargue.csv'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        region 'us-east-2'
        fixedwidth '0:1,1:9,2:9,3:2,4:10'
        dateformat 'mm-dd-yyyy';
        --fixedwidth le pasamos el tama√±o de caracteres de cada una de las columnas
        --primera(0) de un caracter, segunda(1) de nueve caracteres, etc.
        
        select * from estudiante;
        
        --para MANIFEST trabajaremos con el otro repo y cargaremos dichos archivos
        --manifest es un archivo de formato de json que le configuraremos la ruta de los cargos
        --e incluso si es obligatoria su subida
        --{
        --    "entries": [
        --        {"url":"s3://mibucketredshiftsalda/cargue_1.csv", "mandatory":true},
        --        {"url":"s3://mibucketredshiftsalda/cargue_2.csv", "mandatory":true}
        --    ]
        --}
        --OJO: vuelve a subir el test.manifest actualizado
        
        --damos la instrucci√≥n de cargue
        copy estudiante from 's3://mibucketredshiftsalda/test.manifest'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        delimiter ';'
        ignoreheader 1
        manifest
        region 'us-east-2';
        
        select * from estudiante;
        
        --Lo interesante adem√°s de cargar desde un solo archivo a varios es que estos pueden estar en rutas y buckets distintos
        --Recuerda la buena pr√°ctica de particionar los archivos para una mejor gesti√≥n de los recursos
        
        --finalmente veremos el comando de compresi√≥n COMPUPDATE
        --esto lo haremos con la tabla sales
        --creamos dos versiones una con compression on y otra en off
        --y les pasaremos los datos de sales con copy
        CREATE TABLE IF NOT EXISTS public.sales_compression_on
        (
        	salesid INTEGER NOT NULL  ENCODE raw
        	,listid INTEGER NOT NULL  ENCODE raw
        	,sellerid INTEGER NOT NULL  ENCODE raw
        	,buyerid INTEGER NOT NULL  ENCODE raw
        	,eventid INTEGER NOT NULL  ENCODE raw
        	,dateid SMALLINT NOT NULL  ENCODE raw
        	,qtysold SMALLINT NOT NULL  ENCODE raw
        	,pricepaid NUMERIC(8,2)   ENCODE raw
        	,commission NUMERIC(8,2)   ENCODE raw
        	,saletime TIMESTAMP WITHOUT TIME ZONE   ENCODE raw
        );
        
        CREATE TABLE IF NOT EXISTS public.sales_compression_off
        (
        	salesid INTEGER NOT NULL  ENCODE raw
        	,listid INTEGER NOT NULL  ENCODE raw
        	,sellerid INTEGER NOT NULL  ENCODE raw
        	,buyerid INTEGER NOT NULL  ENCODE raw
        	,eventid INTEGER NOT NULL  ENCODE raw
        	,dateid SMALLINT NOT NULL  ENCODE raw
        	,qtysold SMALLINT NOT NULL  ENCODE raw
        	,pricepaid NUMERIC(8,2)   ENCODE raw
        	,commission NUMERIC(8,2)   ENCODE raw
        	,saletime TIMESTAMP WITHOUT TIME ZONE   ENCODE raw
        );
        
        copy sales_compression_on from 's3://mibucketredshiftsalda/tickitdb/sales_tab.txt'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        delimiter '\t' timeformat 'MM/DD/YYYY HH:MI:SS' COMPUPDATE ON
        region 'us-east-2';
        
        --compupdate on nos cambia la estructura de la tabla si est√° vac√≠a
        --a la compresi√≥n que mejor corresponda a esos datos
        
        copy sales_compression_off from 's3://mibucketredshiftsalda/tickitdb/sales_tab.txt'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        delimiter '\t' timeformat 'MM/DD/YYYY HH:MI:SS' COMPUPDATE OFF
        region 'us-east-2';
        
        select * from PG_TABLE_DEF
        where TABLENAME = 'sales_compression_off';
        --as√≠ las sube sin encoding
        
        select * from PG_TABLE_DEF
        where TABLENAME = 'sales_compression_on';
        --as√≠ se actualizan al encoding mejor a pesar de que los subimos en raw
        
        --Esto lo realiza a trav√©s de slices y test de nuestra data 
        --para finalmente hacer el cargue entero
        ```
        
    - **Cargando archivos tipo JSON**
        
        En esta clase conoceremos c√≥mo cargar datos en formato json a Redshift con el comando Copy, ya conoces las ventajas de este comando, ahora veremos c√≥mo llevar nuestros datos a Redshift dado el caso que nuestro origen de datos se encuentre en un formato json.
        
        Lo primero que debemos saber es que existen dos formas de cargar los datos en formato json en Redshift, una es la forma autom√°tica, en donde cada ‚Äúkey‚Äù del archivo json debe hacer match con cada columna que queramos llenar de una tabla en Redshift.
        
        Es importante saber que las columnas en Redshift se crean en min√∫sculas, de manera que las ‚Äúkey‚Äù del archivo en json deben estar en min√∫scula tambi√©n. La segunda forma es especificando la estructura del archivo para que este pueda ser cargado a la tabla, para ello utilizaremos un archivo extra denominado jsonpaths.
        
        Carga autom√°tica:
        
        Usando la estructura de tabla que hemos manejado en este curso:
        
        ```sql
        --Create table
        createtable  estudiante
        (id  int2,
        nombre  varchar(20),
        apellido  varchar(20),
        edad  int2,
        fecha_ingreso  date );
        ```
        
        Crearemos una estructura en Redshift que pueda satisfacer las necesidades de esta tabla, de modo que puede quedar algo as√≠ (recuerda que los ‚Äúkey‚Äù del archivo se debe llamar igual que en la tabla):
        
        ```json
        {
        	"id": 4544,
        	"nombre": "test_1",
        	"apellido": "json_1",
        	"edad": 33,
        	"fecha_ingreso": "2020-08-01"
        }
        {
        	"id": 23232,
        	"nombre": "test_2",
        	"apellido": "json_2",
        	"edad": 22,
        	"fecha_ingreso": "2020-08-03"
        }
        ```
        
        Ahora lo subimos a S3 con el nombre que queramos y procedemos a cargarlo de la siguiente manera:
        
        ```sql
        copy  public.estudiante  from  's3://[tu_propia_ruta_del_archivo_json]'  credentials  'aws_iam_role=[tu_iam_role]'
        format  as  json  'auto'  region  '[tu_region]';
        ```
        
        Como se ve en la gr√°fica, la tabla se encuentra cargada con los datos que existen en el archivo json.
        
        Carga con jsonpaths:
        
        Esta carga consiste en determinar una estructura basada en el archivo json que pueda ser insertada en la tabla, como sabemos en un archivo json podemos tener datos no estructurados, pero Redshift requiere de data estructurada; de manera que lo primero ser√° crear un peque√±o ejemplo de un archivo json con datos no estructurada.
        
        ```json
        {
        	"id": 4544,
        	"nombre": "test_json3",
        	"apellido": "test_json3",
        	"edad": [
        		24332,
        		33,
        		443,
        		323232,
        		43434
        	],
        	"fechas": [
        		{
        			"id": 0,
        			"fecha_ingreso": "2015-05-01"
        		},
        		{
        			"id": 1,
        			"fecha_ingreso": "2016-05-30"
        		}
        	]
        }
        ```
        
        Vamos a suponer que de la lista de edades son la segunda posici√≥n o sea la posici√≥n [1] es la que yo requiero, y para las fechas, la fecha que en verdad me interesa es la contenida en el segundo bloque, para especificar estas reglas crearemos un nuevo archivo con la siguiente estructura y lo nombraremos ‚Äújsonpaths‚Äù:
        
        ```json
        {
        	"jsonpaths": {
        		"$['id']",
        		"$['nombre']",
        		"$['apellido']",
        		"$['edad'][1]",
        		"$['fechas'][1]['fecha_ingreso']"
        	}
        }
        ```
        
        En donde el s√≠mbolo ($) indica la ra√≠z del archivo json.
        
        Ahora subimos nuestros dos archivos a S3 y ejecutamos la siguiente sentencia en nuestro editor SQL.
        
        ```sql
        copy  public.estudiante  from  's3://[tu_propia_ruta_del_archivo_json]'  credentials  'aws_iam_role=[tu_iam_role]'
        format  as  json  's3://[tu_propia_ruta_del_archivo_jsonpaths]'  region  'tu_propia_region';
        ```
        
        Y como vemos, el registro fue cargado exitosamente.
        
        ![https://static.platzi.com/media/user_upload/redshift-table-4080c993-06ae-4742-a9fa-cb2a3048e40d.jpg](https://static.platzi.com/media/user_upload/redshift-table-4080c993-06ae-4742-a9fa-cb2a3048e40d.jpg)
        
    - **M√©todos de carga alternativos al comando copy**
        
        ## Deep Copy
        
        Una copia profunda recrea y vuelve a llenar una tabla mediante una inserci√≥n masiva, que¬†**ordena autom√°ticamente la tabla**. Si una tabla tiene una regi√≥n grande sin clasificar, una copia profunda es mucho m√°s r√°pida que una aspiradora. La compensaci√≥n es que no debe realizar actualizaciones simult√°neas durante una operaci√≥n de copia profunda a menos que pueda rastrearla y mover las actualizaciones delta a la nueva tabla despu√©s de que se haya completado el proceso. Una operaci√≥n de VAC√çO admite actualizaciones simult√°neas de forma autom√°tica.
        
        Puede elegir uno de los siguientes m√©todos para crear una copia de la tabla original:
        
        - Utilice la tabla DDL original.
        
        Si el DDL CREATE TABLE est√° disponible, este es el m√©todo m√°s r√°pido y preferido. Si crea una tabla nueva, puede especificar todos los atributos de tabla y columna, incluidas la clave principal y las claves externas.
        
        - Utilice CREAR TABLA COMO.
        
        Si el DDL original no est√° disponible, puede usar CREATE TABLE LIKE para recrear la tabla original. La nueva tabla hereda los atributos encoding, distkey, sortkey y notnull de la tabla principal. La nueva tabla no hereda los atributos de clave principal y clave externa de la tabla principal, pero puede agregarlos usando¬†[ALTER TABLE.](https://docs.aws.amazon.com/redshift/latest/dg/r_ALTER_TABLE.html)
        
        - Cree una tabla temporal y trunque la tabla original.
        
        Si necesita conservar la clave principal y los atributos de clave externa de la tabla principal, o si la tabla principal tiene dependencias, puede usar CREATE TABLE ‚Ä¶ AS (CTAS) para crear una tabla temporal, luego truncar la tabla original y completar de la tabla temporal.
        
        El uso de una tabla temporal mejora el rendimiento significativamente en comparaci√≥n con el uso de una tabla permanente, pero existe el riesgo de perder datos. Una tabla temporal se elimina autom√°ticamente al final de la sesi√≥n en la que se crea. TRUNCATE se compromete inmediatamente, incluso si est√° dentro de un bloque de transacci√≥n. Si TRUNCATE tiene √©xito pero la sesi√≥n finaliza antes de que finalice el INSERT subsiguiente, los datos se pierden. Si la p√©rdida de datos es inaceptable, use una tabla permanente.
        
        **Para realizar una copia profunda utilizando la tabla original DDL**
        
        1. (Opcional) Vuelve a crear la tabla DDL ejecutando un script llamado v_generate_tbl_ddl.
        2. Crea una copia de la tabla usando el CREATE TABLE DDL original.
        3. Utilisa una instrucci√≥n INSERT INTO ‚Ä¶ SELECT para completar la copia con datos de la tabla original.
        4. Suelta la mesa original.
        5. Utilisa una instrucci√≥n ALTER TABLE para cambiar el nombre de la copia al nombre de la tabla original.
        
        El siguiente ejemplo realiza una copia profunda en la tabla SALES usando un duplicado de SALES llamado SALESCOPY.
        
        ```sql
        createtable salescopy ( ‚Ä¶ );
        insertinto salescopy (select *from sales);
        droptable sales;
        altertable salescopyrenameto sales;
        ```
        
        **Para realizar una copia profunda usando CREATE TABLE LIKE**
        
        1. Cree una nueva tabla usando CREATE TABLE LIKE.
        2. Utilice una instrucci√≥n INSERT INTO ‚Ä¶ SELECT para copiar las filas de la tabla actual a la nueva tabla.
        3. Suelta la tabla actual.
        4. Utilice una instrucci√≥n ALTER TABLE para cambiar el nombre de la nueva tabla al nombre de la tabla original.
        5. El siguiente ejemplo realiza una copia profunda en la tabla SALES usando CREATE TABLE LIKE.
        
        ```sql
        createtable likesales (like sales);
        insertinto likesales (select *from sales);
        droptable sales;
        altertable likesalesrenameto sales;
        ```
        
        **Para realizar una copia en profundidad creando una tabla temporal y truncando la tabla original**
        
        1. Utilice CREATE TABLE AS para crear una tabla temporal con las filas de la tabla original.
        2. Truncar la tabla actual.
        3. Utilice una instrucci√≥n INSERT INTO ‚Ä¶ SELECT para copiar las filas de la tabla temporal a la tabla original.
        4. Suelta la mesa temporal.
        
        El siguiente ejemplo realiza una copia profunda en la tabla SALES creando una tabla temporal y truncando la tabla original:
        
        ```sql
        create temptable salestempasselect *from sales;
        truncate sales;
        insertinto sales (select *from salestemp);
        droptable salestemp;
        ```
        
        C√≥digo de la clase:
        
        ```sql
        SELECT * from estudiante;
        
        insert into estudiante values
        (5,'Brandon','Huertas',24,'2020-01-01'),
        (5,'Cristian','Salas',24,'2020-01-01'),
        (5,'Holman','Campos',24,'2020-01-01'),
        (5,'Natalia','Montenegro',24,'2020-01-01');
        --no hagas un insert por cada fila, llevalos juntos desde un solo insert 
        
        --bulk select/insert
        select e.eventname , e.starttime, 
        sum(pricepaid) pricepaid, sum(commission) commission
        from sales s 
        inner join event e 
        on s.eventid = e.eventid
        group by e.eventname, e.starttime;
        
        --creamos una tabla como resultado del query anterior
        -- a esto se le conoco como bulk insert por lotes
        create table total_price_by_event as (
        select e.eventname , e.starttime, 
        sum(pricepaid) pricepaid, sum(commission) commission
        from sales s 
        inner join event e 
        on s.eventid = e.eventid
        group by e.eventname, e.starttime
        );
        
        select * from total_price_by_event;
        
        -- deep copy
        create table likesales (like sales);
        insert into likesales (select * from sales);
        drop table sales;
        
        alter table likesales rename to sales;
        
        select * from sales;
        
        -- un uso interesante es probar una configuraci√≥n de compresi√≤n en una tabla, pasar la configuraci√≥n a otra diferente
        -- se borra la original y se renombra a la segunda mejorada como la original
        -- de tal manera que todo el mundo sigue trabajando como si nada hubiese pasado üëÄ
        ```
        
    - **¬øC√≥mo ejecutar sentencias UPDATE y DELETE?**
        
        y si debemos actualizar nuestros datos que ten√≠an alg√∫n error
        
        Vamos a suponer un error en la tabla sales en precio y comisi√≥n de ciertos eventos
        
        ```sql
        select * from sales limit 10;
        
        select * from sales s 
        where eventid in (
        select eventid from event e 
        where eventname = 'Beck');
        
        --nos trae las  ventas con Beck
        
        -- ahora vamos a traer solo las columnas que queremos corregir
        -- simulamos que tienen el doble de precio y eso es lo que debemos actualizar
        create table sales_auxiliar as (
        select salesid, pricepaid * 2 pricepaid , commission *2 commission from sales s 
        where eventid in (
        select eventid from event e 
        where eventname = 'Beck'));
        
        --verificamos
        select * from sales_auxiliar 
        where salesid = 137519
        limit 10;
        
        select * from sales
        where salesid = 137519
        limit 10;
        
        --update 
        update sales 
        set pricepaid = sa.pricepaid
        from sales_auxiliar sa
        where sales.salesid = sa.salesid;
        --con este update los datos del precio en auxiliar se actualizan en sales
        
        --borramos los datos en sales
        delete from sales 
        using sales_auxiliar
        where sales.salesid = sales_auxiliar.salesid;
        
        --actualizamos los datos
        -- debemos traerle todos los datos
        -- primero borramos la auxiliar que tiene solo ciertas columnas
        -- y la recreamos con todos los componentes del sales original.
        drop table sales_auxiliar;
        
        create table sales_auxiliar as (
        select * from sales s 
        where eventid in (
        select eventid from event e 
        where eventname = 'Macbeth'));
        
        delete from sales
        using sales_auxiliar
        where sales.salesid = sales_auxiliar.salesid;
        
        insert into sales (select * from sales_auxiliar);
        
        select * from sales_auxiliar
        where salesid = 111878
        limit 10;
        
        select * from sales
        where salesid = 111878
        limit 10;
        
        --comprobado, borrando los datos cruzados entre ambas tablas para encontrar las diferencias
        -- dichas diferencias las conservamos, borramos la tabla original e insertamos
        -- este es un m√©todo de eficiente de actualizar nuestros datos
        ```
        
    - **¬øC√≥mo mantener el desempe√±o de tu base de datos?**
        
        ## Analyze
        
        La operaci√≥n ANALYZE actualiza los metadatos estad√≠sticos que el planificador de consultas usa para seleccionar planes
        √≥ptimos.
        La operaci√≥n de Analyze consume muchos recursos del sistema, por eso si no han cambiado determinado porcentaje de filas en la tabla no se ejecuta.
        
        El siguiente comando define el porcentaje de cambio en las tablas para analyze.
        
        set analyze_threshold_percent to 10;
        
        El an√°lisis de estad√≠sticas puede ser por toda la base de datos, tablas en espec√≠fico o columnas espec√≠ficas
        
        ## Vacuum (Limpieza)
        
        Reordena las filas y recupera espacio en una tabla especificada o en todas las tablas de la base de datos actual.
        **Tipo de Vacuum:**
        
        FULL
        
        SORT ONLY
        
        DELETE ONLY
        
        REINDEX
        
        Vacuum indispone las tablas de modo que hay que ejecutarlo en una ventana de cero transacciones.
        VACUUM omite la fase de ordenaci√≥n para cualquier tabla que tenga m√°s del 95 por ciento de las filas de la tabla
        ordenadas.
        
        vacuum sort only sales to 85 percent;
        
        **Vacuum (Sort)**
        
        ![figures/Untitled%2034.png](figures/Untitled%2034.png)
        
        ![figures/Untitled%2035.png](figures/Untitled%2035.png)
        
        ![figures/Untitled%2036.png](figures/Untitled%2036.png)
        
        ## Trabajar con autovacuum de PostgreSQL en Amazon RDS
        
        Recomendamos encarecidamente que utilice la funci√≥n de autovacuum para las bases de datos de PostgreSQL para mantener la salud de su instancia de base de datos de PostgreSQL. Autovacuum automatiza el inicio de los comandos VACUUM y ANALYZE. Autovacuum comprueba las tablas que han tenido una gran cantidad de tuplas insertadas, actualizadas o eliminadas. Luego, Autovacuum recupera el almacenamiento eliminando los datos obsoletos o las tuplas de la base de datos de PostgreSQL.
        
        Autovacuum est√° habilitada de forma predeterminada para todas las nuevas instancias de base de datos PostgreSQL de Amazon RDS, y los par√°metros de configuraci√≥n de autovacuum relacionados se establecen de forma predeterminada. Debido a que nuestros valores predeterminados son algo gen√©ricos, puede beneficiarse de ajustar los par√°metros a su carga de trabajo espec√≠fica. La siguiente¬†**[secci√≥n](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.html#Appendix.PostgreSQL.CommonDBATasks.Autovacuum)**¬†puede ayudarte a realizar el ajuste de autovacuum necesario.
        
    - **Estad√≠sticas y limpieza de las tablas**
        
        ```sql
        --practicaremos con una tabla propia de redshift
        
        select * from pg_catalog.pg_statistic_indicator;
        
        select * from stv_tbl_perm
        where name = 'sales';
        
        select * from pg_catalog.pg_statistic_indicator
        where stairelid = 102019;
        
        --analyze por columnas, columnas de predicado o total
        
        select * from pg_table_def
        where tablename = 'sales';
        
        analyze sales(salesid, pricepaid);
        
        analyze sales predicate columns;
        
        analyze sales;
        
        --analisis total de nuestras tablas
        select * from stl_analyze;
        
        --vacuum--
        
        select "table", unsorted, vacuum_sort_benefit from svv_table_info;
        --nos indica qu√© tan desordenada est√° y el beneficio de ordenarla
        
        --vamos a ordenar nuestra tabla sales
        
        --limpieza siempre
        vacuum sales;
        
        -- ordenamiento a partir de un porcentaje
        vacuum sort only sales to 75 percent;
        
        -- borrado a partir de un porcentaje
        vacuum delete only sales to 75 percent;
        
        --renovar los √≠ndices solo funciona con reordenamiento intercalado
        vacuum reindex sales;
        ```
        
- **Buenas pr√°cticas para dise√±ar y ejecutar consultas en tu base de datos**
    - **Agrupamiento, ordenamiento y subqueries**
        - Evita usar ‚Äúselect * from‚Ä¶‚Äù
        - Usar joins por llaves siempre que sea posible (a trav√©s de llaves).
        - Tantas condiciones en el where como sea posible.
        - Evita usar funciones en el select.
        - Usar columnas ‚Äúsort‚Äù en el group by (mismo orden).
        - Usa subconsultas con menos de 200 registros.
        - Si usas group by y order by aseg√∫rate que est√©n en el mismo orden.
        - Filtra lo mismo cuentas veces se pueda en las distintas tablas.
    - **¬øQu√© es y c√≥mo interpretar un explain plan?**
        
        ## Explain Distribution
        
        **DS_DIST_NONE y DS_DIST_ALL_NONE**¬†son buenos indicadores. Indican que no fue necesario redistribuir para ese paso porque todas las combinaciones est√°n ubicadas juntas.
        
        **DS_DIST_INNER**¬†implica que el paso probablemente tiene un costo relativamente alto porque se est√° redistribuyendo la tabla interna a los nodos. DS_DIST_INNER indica que la tabla externa ya est√° distribuida correctamente en la clave de combinaci√≥n. Configure la clave de distribuci√≥n de la tabla interna con la clave de combinaci√≥n para convertirla en DS_DIST_NONE.
        
        Puede utilizar el plan de consulta para obtener informaci√≥n acerca de las operaciones individuales necesarias para ejecutar una consulta. Antes de trabajar con un plan de consulta, le recomendamos que primero comprenda c√≥mo Amazon Redshift administra el procesamiento de consultas y crea planes de consultas. Para obtener m√°s informaci√≥n, consulte Flujo de trabajo de planificaci√≥n y ejecuci√≥n de consultas.
        
        Para crear un plan de consulta, ejecute el comando EXPLAIN seguido del texto real de la consulta. En el plan de consulta, se proporciona la siguiente informaci√≥n:
        
        Las operaciones que realizar√° el motor de ejecuci√≥n, leyendo los resultados de abajo arriba.
        
        El tipo de paso que realiza cada operaci√≥n.
        
        Las tablas y las columnas que se utilizan en cada operaci√≥n.
        
        La cantidad de datos que se procesa en cada operaci√≥n, en cuanto a la cantidad de filas y al ancho de datos en bytes.
        
        El costo relativo de la operaci√≥n. El costo es una medida que compara los tiempos relativos de ejecuci√≥n de los pasos de un plan. El costo no proporciona informaci√≥n precisa acerca de los tiempo de ejecuci√≥n reales ni sobre el consumo de memoria, como tampoco proporciona una comparaci√≥n significativa entre los planes de ejecuci√≥n. El costo le da una idea de cu√°les son las operaciones de una consulta que est√°n consumiendo la mayor cantidad de recursos.
        
        El comando EXPLAIN no ejecuta propiamente la consulta. Solo muestra el plan que Amazon Redshift ejecuta si la consulta se ejecuta en las condiciones de funcionamiento actuales. Si cambia el esquema o los datos de una tabla y ejecuta nuevamente el comando ANALYZE para actualizar los metadatos estad√≠sticos, el plan de consulta podr√≠a ser diferente.
        
        La salida del plan de consulta que genera el comando EXPLAIN es una vista general simplificada de la ejecuci√≥n de consultas. No muestra los detalles del procesamiento en paralelo de las consultas. Para ver informaci√≥n detallada, ejecute la consulta en cuesti√≥n y, luego, obtenga informaci√≥n resumida de la consulta desde la vista SVL_QUERY_SUMMARY o SVL_QUERY_REPORT. Para obtener m√°s informaci√≥n acerca de c√≥mo usar estas vistas, consulte An√°lisis del resumen de consultas.
        
        ![figures/Untitled%2037.png](figures/Untitled%2037.png)
        
        ![figures/Untitled%2038.png](figures/Untitled%2038.png)
        
        ```sql
        explain
        SELECT eventid, eventname, event.venueid , venuename
        FROM event, venue;
        
        explain
        SELECT eventid, eventname, event.venueid , venuename
        FROM event, venue
        WHERE event.eventid = venue.venueid;
        
        SELECT * FROM pg_catalog.pg_table_def 
        WHERE tablename IN ('event', 'venue');
        
        --creamos una tabla a partir de la estructura de event
        --pero agregando las keys de distribuci√≥n y ordenamento
        CREATE TABLE IF NOT EXISTS public.event_2
        (
        	eventid INTEGER NOT NULL  ENCODE az64
        	,venueid SMALLINT NOT NULL  ENCODE az64 distkey sortkey
        	,catid SMALLINT NOT NULL  ENCODE az64
        	,dateid SMALLINT NOT NULL  ENCODE RAW
        	,eventname VARCHAR(200)   ENCODE lzo
        	,starttime TIMESTAMP WITHOUT TIME ZONE   ENCODE az64
        );
        
        INSERT INTO event_2 (SELECT * FROM event);
        
        --revisamos con el explain
        explain
        SELECT eventid, eventname, event_2.venueid , venuename
        FROM event_2, venue
        WHERE event_2.eventid = venue.venueid;
        --pesa much√≠simo menos que el original!
        
        analyze event_2
        
        --funciones de agregaci√≥n
        
        explain
        SELECT e.eventname , sum(pricepaid)
        FROM sales s 
        INNER JOIN event e
        ON s.eventid = e.eventid
        GROUP BY e.eventname; 
        -- todos los procesos suman entre si como un √°rbol
        -- de esta forma podemos ver cuales condiciones del query son de m√°s peso.
        
        explain
        SELECT sum(pricepaid)
         FROM sales s;
        
        SELECT * FROM pg_catalog.pg_table_def 
        WHERE tablename IN ('event');
        --event se ordena a trav√©s del eventid
        
        explain
        SELECT * FROM event;
        
        --repitiendo el query anterior a trav√©s del eventid
        explain
        SELECT * FROM event
        ORDER BY eventid;
        --como ordeno sin llave de ordenamiento es super pesado
        
        explain
        SELECT * FROM event
        ORDER BY dateid;
        --con esta llave de ordenamiento es totalmente mejor
        
        --tabla de alerta de eventosm y posibles soluciones de cada query
        SELECT * FROM pg_catalog.stl_alert_event_log ;
        
        SELECT * FROM pg_catalog.stl_query 
        WHERE query = 97661;
        
        SELECT * FROM pg_catalog.stl_query 
        WHERE query = 98675;
        ```
        
- **An√°lisis de comportamiento y descarga de datos con Redshift**
    - **¬øC√≥mo descargar datos eficientemente con UNLOAD?**
        
        Veremos como descargar nuestros datos en redshift
        
        ```sql
        CREATE TABLE unload_test AS (
        SELECT * FROM cust_sales_intercalado 
        UNION ALL
        SELECT * FROM cust_sales_intercalado 
        UNION ALL
        SELECT * FROM cust_sales_intercalado 
        );
        --esto es supremamente ineficiente y una mala pr√°ctica
        
        SELECT COUNT(0) FROM cust_sales_intercalado;
        --50 millones!
        
        SELECT COUNT(0) FROM unload_test;
        --150 millones de datos
        
        unload ('select * from unload_test')
        to 's3://mibucketredshiftsalda/unload/unload_test_'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift';
        --te crea un archivo por cada segmento de datos que tenemos
        
        --si queremos todo en un solo archivo
        --NO RECOMENDABLE
        unload ('select * from unload_test')
        to 's3://mibucketredshiftsalda/unload/unload_test_'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        parallel off
        ALLOWOVERWRITE;
        --nos crea los archivos sobreescribiendo lo que ten√≠amos previamente
        
        --creamos una tabla limitada a 2millones de registros
        CREATE TABLE unload_test_2 AS (
        	SELECT * 
        	FROM unload_test
        	LIMIT 2000000);
        
        --podemos incluirle m√°s par√°metros al archivo que crearemos
        unload ('select * from unload_test_2')
        to 's3://mibucketredshiftsalda/unload/unload_test_'
        credentials 'aws_iam_role=arn:aws:iam::705146719858:role/MiRolRedshift'
        ALLOWOVERWRITE
        delimiter ';'
        header
        maxfilesize 500 mb
        ZSTD
        manifest
        partition by (c_nation) INCLUDE 
        ;
        ```
        
        El par√°metro¬†**MANIFEST**¬†crear un archivo en formato¬†**JSON**¬†con el detalle de las rutas y tama√±o se los archivos creados, ejemplo:
        
        **Nombre de archivo:**¬†unload_test_3manifest
        
        **Contenido**
        
        ```json
        {
          "entries": [
            {"url":"s3://almacenamiento/unload/unload_test_30000_part_00.zst", "meta": { "content_length": 7591016 }},
            {"url":"s3://almacenamiento/unload/unload_test_30001_part_00.zst", "meta": { "content_length": 7589291 }}
          ]
        }
        
        ```
        
        El par√°metro¬†**PARTITION BY**¬†crea los archivos de acuerdo al resultado de la agrupaci√≥n de la tabla por la columna o columnas indicadas (se crear√° una carpeta por cada grupo y se colocar√°n los archivos dentro)Por ejemplo para si en la partici√≥n de especifica la columna C_REGION de la siguiente forma:
        
        ```sql
        unload ('select*from unload_test_2')
        TO 's3://almacenamiento/unload/unload_test_4_'
        credentials 'aws_iam_role=identificador_rol'
        allowoverwrite --Sobreescribir archivos existentes
        delimiter ';' --Delimitador de columnas
        header --Incluir cabecera de columnas
        maxfilesize 500 mb --limitar el tama√±o de archivo a 500 MB
        zstd --crear archivo comprimido
        manifest
        partitionby (c_region) include
        
        ```
        
        Se crear√° la siguiente estructura de archivos en S3:
        
        **Carpetas:**unload_test_4_unload_test_4_/c_region=AFRICAunload_test_4_/c_region=AMERICA
        
        **Archivos:**unload_test_4_/manifestunload_test_4_/c_region=AFRICA/0000_part_00.zstunload_test_4_/c_region=AFRICA/0001_part_00.zstunload_test_4_/c_region=AMERICA/0000_part_00.zstunload_test_4_/c_region=AMERICA/0001_part_00.zst
        
    - **Otras tablas √∫tiles de Redshift para entender el comportamiento de nuestros datos**
        
        ```json
        SELECT * FROM pg_table_def
        WHERE tablename = 'sales'; -- encode, distribucion, ordenamiento
        
        SELECT * FROM pg_catalog.stv_blocklist sb ; -- permite ver los bloques de datos
        
        SELECT * FROM pg_catalog.stl_load_errors ; -- logs de los errores en nuestros copies
        
        SELECT * FROM pg_catalog.stl_load_commits ; -- logs de los commits quedaron bien de nuestros copies
        
        SELECT * FROM stl_query; -- informacion de los queries
        
        SELECT * FROM stl_query
        WHERE query = 29;
        
        SELECT * FROM pg_catalog.svl_qlog  -- ver queries
        ORDER BY starttime desc;
        
        SELECT * FROM pg_catalog.svl_user_info ; -- informacion de los usuarios
        
        CREATE user invitado password 'Password123';
        
        SELECT * FROM svv_tables;
        
        SELECT * FROM svv_tables
        WHERE table_schema = 'public';
        ```
        

[Examen Redshift](https://www.notion.so/Examen-Redshift-677fd4bd469f43ae8e8f1ed20ed5a3a3)
